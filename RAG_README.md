# ğŸ§ª RAG Experiments Report (Exp-1 â†’ Exp-5)

## ğŸ¯ Goal
To evaluate **lightweight Retrieval-Augmented Generation (RAG) pipelines** with small LLMs.  
Focus areas:
- Retrieval quality (keyword vs. embeddings)
- Model compliance with instructions
- Answer precision (copying exact phrases vs. hallucinations)
- Impact of prompt design  

**Corpus:** 15 documents (`docs.jsonl`)  
**Queries:** 15 questions (`questions.json`)

---

## ğŸ“Š Experiment Summaries

### **Experiment 1 â€“ TF-IDF + TinyLlama**
- **Retriever:** TF-IDF (k=5)  
- **LLM:** TinyLlama-1.1B  
- **Prompt:** Strict copy or *â€œI donâ€™t knowâ€*  
- **Settings:** `max_new_tokens=20, temp=0.01`

**Observations**
- âŒ Weak retrieval: missed semantic matches.
- âŒ TinyLlama too small: poor instruction-following, truncated outputs.
- âŒ Prompt fragility: echoed rules, cut answers.  

**Takeaway:**  
TF-IDF not reliable for semantic facts. TinyLlama struggles with strict factual QA.

---

### **Experiment 2 â€“ FAISS + MiniLM + TinyLlama**
- **Retriever:** FAISS + MiniLM (all-MiniLM-L6-v2)  
- **LLM:** TinyLlama-1.1B  
- **Prompt:** Strict copy or *â€œI donâ€™t knowâ€*  

**Observations**
- âœ… Retrieval better: semantically relevant docs.  
- âœ… Fewer repetitions.  
- âŒ Still weak on numeric/config facts.  
- âŒ Truncation & hallucinations remain.  

**Takeaway:**  
FAISS + MiniLM > TF-IDF for general retrieval, but TinyLlama is the bottleneck.

---

### **Experiment 3 â€“ FAISS + MPNet + TinyLlama**
- **Retriever:** FAISS + MPNet (all-mpnet-base-v2)  
- **LLM:** TinyLlama-1.1B  

**Observations**
- âœ… Improved semantic capture (e.g., correct VulcanGraph port: `7787`).  
- âœ… Less redundancy.  
- âŒ Still paraphrasing mistakes & truncations.  
- âŒ Sparse facts depend on `top-k`.  

**Takeaway:**  
MPNet > MiniLM for retrieval, but TinyLlama remains underpowered.

---

### **Experiment 4 â€“ FAISS + MPNet + FLAN-T5-small**
- **Retriever:** FAISS + MPNet  
- **LLM:** FLAN-T5-small (77M, instruction-tuned)  
- **Prompt:** Strict copy or *â€œI donâ€™t knowâ€*  

**Observations**
- âœ… Excellent instruction-following, minimal hallucinations.  
- âœ… More reliable than TinyLlama despite being smaller.  
- âŒ Too strict: frequent *â€œI donâ€™t knowâ€*.  
- âŒ Retrieval failure = answer failure.  

**Takeaway:**  
FLAN-T5-small > TinyLlama for factual QA, but overly strict prompt reduced coverage.

---

### **Experiment 5 â€“ FAISS + MPNet + FLAN-T5-small (Liberal Prompt)**
- **Retriever:** FAISS + MPNet  
- **LLM:** FLAN-T5-small  
- **Prompt:** Liberal factual prompt (exact phrases for features, slight flexibility for numeric values).  

**Observations**
- âœ… Balanced strictness & flexibility â†’ fewer unnecessary *â€œI donâ€™t knowâ€*.  
- âœ… Precise answers (e.g., `port 7787`, *â€œproperty graph database with Gremlin-like traversalâ€*).  
- âœ… Best coverage for both qualitative & numeric facts.  
- âŒ Still some truncation for complex numeric schemes.  
- âŒ Retrieval errors remain a bottleneck.  

**Takeaway:**  
Liberal prompt unlocked FLAN-T5â€™s potential â†’ best overall setup.

---

## ğŸ“ˆ Comparative Overview

| Aspect                 | Exp-1       | Exp-2            | Exp-3            | Exp-4              | Exp-5               |
|-------------------------|-------------|------------------|------------------|--------------------|---------------------|
| **Retriever**           | TF-IDF      | FAISS + MiniLM   | FAISS + MPNet    | FAISS + MPNet      | FAISS + MPNet       |
| **LLM**                 | TinyLlama   | TinyLlama        | TinyLlama        | FLAN-T5-small      | FLAN-T5-small       |
| **Prompt**              | Strict      | Strict           | Strict           | Strict             | Liberal             |
| **Retriever Quality**   | Weak        | Good             | Best             | Best               | Best                |
| **Instruction Following** | Poor      | Poor             | Poor             | Strong             | Strong              |
| **Numeric Accuracy**    | Fails       | Fails            | Sometimes        | Missed literals    | Improved            |
| **Hallucinations**      | Frequent    | Some             | Some             | Rare               | Rare                |
| **â€œI donâ€™t knowâ€ Usage** | Low        | Low              | Low              | High               | Balanced            |
| **Overall Performance** | ğŸš«          | âš ï¸               | âš ï¸+              | âœ… (strict)        | ğŸŒŸ Best balance     |

---

## ğŸ¯ Final Conclusions

### ğŸ”¹ Retriever Evolution
- **TF-IDF** â†’ noisy, weak.  
- **MiniLM** â†’ decent but limited.  
- **MPNet** â†’ best semantic retrieval.  

### ğŸ”¹ LLM Role
- **TinyLlama-1.1B** â†’ too small, weak for factual QA.  
- **FLAN-T5-small** â†’ far smaller, but instruction-tuned â†’ much better.  

### ğŸ”¹ Prompting Matters
- **Strict prompts** â†’ higher accuracy, lower coverage.  
- **Liberal prompts** â†’ balance between factuality & usability.  

---

## âœ… Best Setup (Exp-5)
**FAISS + MPNet + FLAN-T5-small + Liberal Prompt**  
â†’ Most practical and precise balance of **accuracy, coverage, and reliability**.  

---

---

## ğŸ”¹ Framework & Modularity

For these experiments, I **used LangChain** to manage the RAG pipeline.  
This enabled:
- Modular and reusable code for different retrievers and LLMs
- Easy swapping of embeddings (MiniLM, MPNet) and LLMs (TinyLlama, FLAN-T5-small)
- Rapid experimentation with prompt styles and retrieval strategies
- Cleaner orchestration of retrieval + generation steps  

Using LangChain allowed me to **iterate quickly**, test multiple configurations, and maintain **readable, reusable code** for future RAG experiments.


### ğŸ“Œ Key Lesson
For **lightweight RAG**:
- **Retriever:** Embedding-based (FAISS + MPNet)  
- **LLM:** Instruction-tuned (FLAN-T5-small)  
- **Prompt:** Balanced liberal prompt  

This combination gives the **sweet spot** for factual QA with small models.
